{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"3.5.3_수정.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"}},"cells":[{"cell_type":"code","metadata":{"id":"JQR0C6UEG1Pf","executionInfo":{"status":"ok","timestamp":1626887899283,"user_tz":-540,"elapsed":4305,"user":{"displayName":"홍승백","photoUrl":"","userId":"17288301376861649879"}}},"source":["# 실습에 필요한 라이브러리를 불러옵니다.\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import vocab\n","from torchtext import datasets\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.utils.data import DataLoader\n","from collections import Counter\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"tmbn-5sHHEOh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626887929166,"user_tz":-540,"elapsed":29913,"user":{"displayName":"홍승백","photoUrl":"","userId":"17288301376861649879"}},"outputId":"6ff35f17-ce93-493f-bb3b-543cc894f6be"},"source":["# IMDB 데이터세트의 학습 세트를 불러옵니다.\n","train_dataset = datasets.IMDB(split=('train'))"],"execution_count":2,"outputs":[{"output_type":"stream","text":["aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:03<00:00, 22.6MB/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"zE78_95gIFcE","executionInfo":{"status":"ok","timestamp":1626887929168,"user_tz":-540,"elapsed":8,"user":{"displayName":"홍승백","photoUrl":"","userId":"17288301376861649879"}}},"source":["# 토크나이저 객체를 생성합니다.\n","tokenizer = get_tokenizer('basic_english')"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"MLNdS0eg6GEL","executionInfo":{"status":"ok","timestamp":1626887933573,"user_tz":-540,"elapsed":4411,"user":{"displayName":"홍승백","photoUrl":"","userId":"17288301376861649879"}}},"source":["# 단어별 누적 사용 빈도를 계산하기 위해 카운터 객체를 생성합니다.\n","counter = Counter()\n","\n","# 학습 세트의 문장을 단어 단위로 토큰화하고 단어별 누적 사용 빈도를 계산합니다.\n","for (label, text) in train_dataset:\n","    # 문장을 단어 단위로 토큰화하고 단어별로 사용 빈도를 기록합니다.\n","    counter.update(tokenizer(text))"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"t1ckdrY9J7fv","executionInfo":{"status":"ok","timestamp":1626887933577,"user_tz":-540,"elapsed":12,"user":{"displayName":"홍승백","photoUrl":"","userId":"17288301376861649879"}}},"source":["# 10번 이상 사용된 단어를 사용해서 단어장을 만듭니다.\n","vocabulary = vocab(counter, min_freq=10)\n","vocabulary.set_default_index(0)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"zr2hzx1mK4G7","executionInfo":{"status":"ok","timestamp":1626887933578,"user_tz":-540,"elapsed":10,"user":{"displayName":"홍승백","photoUrl":"","userId":"17288301376861649879"}}},"source":["# 텍스트를 정수 인코딩하는 람다 함수를 정의합니다.\n","text_transform = lambda x: [vocabulary[token] for token in tokenizer(x)]\n","\n","# 레이블을 정숫값으로 치환하는 람다 함수를 정의합니다.\n","label_transform = lambda x: 1 if x == 'pos' else 0"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"vOJoO93kjXKt","executionInfo":{"status":"ok","timestamp":1626887933578,"user_tz":-540,"elapsed":8,"user":{"displayName":"홍승백","photoUrl":"","userId":"17288301376861649879"}}},"source":["# 방금 정의한 두 개의 람다 함수를 이용해 텍스트와 레이블을 전처리하는 함수를 정의합니다.\n","def preprocessing(batch):\n","    label_list, text_list = [], []\n","    \n","    # 람다 함수를 사용해서 배치값을 차례대로 변환합니다.\n","    for (_label, _text) in batch:\n","        # 레이블에 람다 함수를 적용합니다.\n","        label_list.append(label_transform(_label))\n","        # 텍스트에 람다 함수를 적용합니다.\n","        text_list.append(torch.tensor(text_transform(_text)))\n","        \n","    # 가장 긴 문장을 기준으로 정수 인코딩 된 문장의 길이를 통일합니다.\n","    data = pad_sequence(text_list)\n","    target = torch.tensor(label_list)\n","    \n","    # 전처리 결과를 반환합니다.\n","    return data, target"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"KMkwXgY1lUXl","executionInfo":{"status":"ok","timestamp":1626887950364,"user_tz":-540,"elapsed":16793,"user":{"displayName":"홍승백","photoUrl":"","userId":"17288301376861649879"}}},"source":["# IMDB 데이터세트를 학습 세트와 테스트 세트로 나눠서 불러옵니다.\n","train_dataset, test_dataset = datasets.IMDB(split=('train', 'test'))\n","\n","# preprocessing 함수를 적용하여 학습 세트 데이터로더와 테스트 세트 데이터로더를 만듭니다.\n","train_loader = DataLoader(list(train_dataset), batch_size=8, shuffle=True, collate_fn=preprocessing)\n","test_loader = DataLoader(list(test_dataset), batch_size=8, shuffle=False, collate_fn=preprocessing)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kfzf_HslGa8R","executionInfo":{"status":"ok","timestamp":1626887950368,"user_tz":-540,"elapsed":15,"user":{"displayName":"홍승백","photoUrl":"","userId":"17288301376861649879"}}},"source":["# LSTM 모델 클래스를 정의합니다.\n","class LSTM(nn.Module):\n","    def __init__(self, vocab_size):\n","        super().__init__()\n","        # 모델 구조를 정의합니다.\n","        self.embed = nn.Embedding(vocab_size, 16)\n","        self.cell = nn.LSTM(16, 16)\n","        self.fc = nn.Linear(16, 1)\n","        self.sigmoid = nn.Sigmoid()\n","        \n","    # 순전파를 정의합니다.\n","    def forward(self, X):\n","        out = self.embed(X)\n","        out, (hidden_state, cell_state) = self.cell(out)\n","        out = self.fc(hidden_state.view(-1, 16))\n","        out = self.sigmoid(out)\n","        return out"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"C94z4efYK2KD","executionInfo":{"status":"ok","timestamp":1626887962249,"user_tz":-540,"elapsed":11894,"user":{"displayName":"홍승백","photoUrl":"","userId":"17288301376861649879"}}},"source":["# 그래픽 카드 사용이 가능할 경우 그래픽 카드로 연산하도록 설정합니다.\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","# LSTM 모델 객체를 생성합니다.\n","vocab_size = len(vocabulary)\n","model = LSTM(vocab_size).to(device)\n","\n","# 이진 크로스 엔트로피(Binary Cross Entropy Error) 손실 함수 객체를 생성합니다.\n","criterion = nn.BCELoss().to(device)\n","\n","# 아담 옵티마이저 객체를 생성합니다.\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"lvqVEySFwjyg","executionInfo":{"status":"ok","timestamp":1626887962250,"user_tz":-540,"elapsed":16,"user":{"displayName":"홍승백","photoUrl":"","userId":"17288301376861649879"}}},"source":["# 학습 함수를 정의합니다.\n","def train(model, criterion, optimizer, loader):\n","    # 현재 에포크의 오차와 정확도를 저장할 변수를 생성합니다.\n","    epoch_loss = 0\n","    epoch_acc = 0\n","\n","    # 모델을 학습 모드로 설정합니다.\n","    model.train()\n","    # 배치 학습을 실행합니다.\n","    for X_batch, y_batch in loader:\n","        # 입력 데이터와 타깃을 준비합니다.\n","        X_batch, y_batch = X_batch.to(device), y_batch.to(device).float().view(-1, 1)\n","        # 기울기를 초기화합니다.\n","        optimizer.zero_grad()\n","        # 모델을 사용해 타깃을 추론합니다.\n","        hypothesis = model(X_batch)\n","        # 손실 함수로 오차를 계산합니다.\n","        loss = criterion(hypothesis, y_batch)\n","        # 기울기를 계산합니다.\n","        loss.backward()\n","        # 경사 하강법으로 가중치를 수정합니다.\n","        optimizer.step()\n","        # 정확도를 계산합니다.\n","        acc = ((hypothesis >= 0.5) == y_batch).float().mean()\n","        # 현재 배치의 오차와 정확도를 저장합니다.\n","        epoch_loss += loss.item()\n","        epoch_acc += acc.item()\n","        \n","    # 현재 에포크의 오차와 정확도를 반환합니다.\n","    return epoch_loss / len(loader), epoch_acc / len(loader)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"_S_sjAtTErMB","executionInfo":{"status":"ok","timestamp":1626887962251,"user_tz":-540,"elapsed":15,"user":{"displayName":"홍승백","photoUrl":"","userId":"17288301376861649879"}}},"source":["# 평가 함수를 정의합니다.\n","def evaluate(model, criterion, optimizer, loader):\n","    # 현재 에포크의 오차와 정확도를 저장할 변수를 생성합니다.\n","    epoch_loss = 0\n","    epoch_acc = 0\n","    \n","    # 모델을 평가 모드로 설정합니다.\n","    model.eval()\n","    with torch.no_grad():\n","        # 배치 단위로 추론을 실행합니다.\n","        for X_batch, y_batch in loader:\n","            # 입력 데이터와 타깃을 준비합니다.\n","            X_batch, y_batch = X_batch.to(device), y_batch.to(device).float().view(-1, 1)\n","            # 모델을 사용해 타깃을 추론합니다.\n","            hypothesis = model(X_batch)\n","            # 손실 함수로 오차를 계산합니다.\n","            loss = criterion(hypothesis, y_batch)\n","            # 정확도를 계산합니다.\n","            acc = ((hypothesis >= 0.5) == y_batch).float().mean()\n","            # 현재 배치의 오차와 정확도를 저장합니다.\n","            epoch_loss += loss.item()\n","            epoch_acc += acc.item()\n","        \n","    # 현재 에포크의 오차와 정확도를 반환합니다.\n","    return epoch_loss / len(loader), epoch_acc / len(loader)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LOPQn5LBErMC","executionInfo":{"status":"ok","timestamp":1626889979061,"user_tz":-540,"elapsed":2016823,"user":{"displayName":"홍승백","photoUrl":"","userId":"17288301376861649879"}},"outputId":"61de777d-458e-42f3-8594-4fc03b6cbe67"},"source":["# 25회에 걸쳐 모델을 학습합니다.\n","n_epochs = 25\n","for epoch in range(n_epochs):\n","    # 모델을 학습시킵니다.\n","    loss, acc = train(model, criterion, optimizer, train_loader)\n","    # 모델을 평가합니다.\n","    test_loss, test_acc = evaluate(model, criterion, optimizer, test_loader)\n","    \n","    # 현재 에포크의 학습 결과를 출력합니다.\n","    print('epoch: {}, loss: {:.3f}, acc: {:.2f}, test_loss: {:.3f}, test_acc: {:.3f}'.format(\n","        epoch, loss, acc, test_loss, test_acc\n","    ))"],"execution_count":13,"outputs":[{"output_type":"stream","text":["epoch: 0, loss: 0.693, acc: 0.50, test_loss: 0.692, test_acc: 0.509\n","epoch: 1, loss: 0.689, acc: 0.52, test_loss: 0.690, test_acc: 0.511\n","epoch: 2, loss: 0.685, acc: 0.52, test_loss: 0.688, test_acc: 0.514\n","epoch: 3, loss: 0.678, acc: 0.52, test_loss: 0.682, test_acc: 0.524\n","epoch: 4, loss: 0.673, acc: 0.53, test_loss: 0.679, test_acc: 0.527\n","epoch: 5, loss: 0.666, acc: 0.54, test_loss: 0.680, test_acc: 0.523\n","epoch: 6, loss: 0.659, acc: 0.55, test_loss: 0.684, test_acc: 0.538\n","epoch: 7, loss: 0.601, acc: 0.66, test_loss: 0.573, test_acc: 0.726\n","epoch: 8, loss: 0.528, acc: 0.76, test_loss: 0.572, test_acc: 0.749\n","epoch: 9, loss: 0.508, acc: 0.79, test_loss: 0.542, test_acc: 0.769\n","epoch: 10, loss: 0.617, acc: 0.61, test_loss: 0.649, test_acc: 0.549\n","epoch: 11, loss: 0.498, acc: 0.77, test_loss: 0.487, test_acc: 0.793\n","epoch: 12, loss: 0.530, acc: 0.75, test_loss: 0.620, test_acc: 0.679\n","epoch: 13, loss: 0.551, acc: 0.73, test_loss: 0.502, test_acc: 0.773\n","epoch: 14, loss: 0.411, acc: 0.83, test_loss: 0.549, test_acc: 0.721\n","epoch: 15, loss: 0.366, acc: 0.86, test_loss: 0.433, test_acc: 0.830\n","epoch: 16, loss: 0.308, acc: 0.89, test_loss: 0.406, test_acc: 0.836\n","epoch: 17, loss: 0.282, acc: 0.90, test_loss: 0.453, test_acc: 0.825\n","epoch: 18, loss: 0.398, acc: 0.82, test_loss: 0.419, test_acc: 0.818\n","epoch: 19, loss: 0.246, acc: 0.91, test_loss: 0.375, test_acc: 0.854\n","epoch: 20, loss: 0.215, acc: 0.92, test_loss: 0.376, test_acc: 0.854\n","epoch: 21, loss: 0.186, acc: 0.93, test_loss: 0.368, test_acc: 0.846\n","epoch: 22, loss: 0.172, acc: 0.94, test_loss: 0.393, test_acc: 0.863\n","epoch: 23, loss: 0.151, acc: 0.95, test_loss: 0.389, test_acc: 0.861\n","epoch: 24, loss: 0.134, acc: 0.95, test_loss: 0.403, test_acc: 0.863\n"],"name":"stdout"}]}]}