{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JQR0C6UEG1Pf"
   },
   "outputs": [],
   "source": [
    "# 실습에 필요한 라이브러리를 불러옵니다.\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import vocab\n",
    "from torchtext import datasets\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tmbn-5sHHEOh",
    "outputId": "bd99e244-672e-4a15-f4d6-1e78d24d5644"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:00<00:00, 92.7MB/s]\n"
     ]
    }
   ],
   "source": [
    "# IMDB 데이터세트의 학습 세트를 불러옵니다.\n",
    "train_dataset = datasets.IMDB(split=('train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "zE78_95gIFcE"
   },
   "outputs": [],
   "source": [
    "# 토크나이저 객체를 생성합니다.\n",
    "tokenizer = get_tokenizer('basic_english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MLNdS0eg6GEL"
   },
   "outputs": [],
   "source": [
    "# 단어별 누적 사용 빈도를 계산하기 위해 카운터 객체를 생성합니다.\n",
    "counter = Counter()\n",
    "\n",
    "# 학습 세트의 문장을 단어 단위로 토큰화하고 단어별 누적 사용 빈도를 계산합니다.\n",
    "for (label, text) in train_dataset:\n",
    "    # 문장을 단어 단위로 토큰화하고 단어별로 사용 빈도를 기록합니다.\n",
    "    counter.update(tokenizer(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "t1ckdrY9J7fv"
   },
   "outputs": [],
   "source": [
    "# 10번 이상 사용된 단어를 사용해서 단어장을 만듭니다.\n",
    "vocabulary = vocab(counter, min_freq=10)\n",
    "vocabulary.set_default_index(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-hv5BaUQ3Yiu",
    "outputId": "bb3bf19e-8367-4f99-896c-b6e9b21914bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1395"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary['hello']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "zr2hzx1mK4G7"
   },
   "outputs": [],
   "source": [
    "# 텍스트를 정수 인코딩하는 람다 함수를 정의합니다.\n",
    "text_transform = lambda x: [vocabulary[token] for token in tokenizer(x)]\n",
    "\n",
    "# 레이블을 정숫값으로 치환하는 람다 함수를 정의합니다.\n",
    "label_transform = lambda x: 1 if x == 'pos' else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "vOJoO93kjXKt"
   },
   "outputs": [],
   "source": [
    "# 방금 정의한 두 개의 람다 함수를 이용해 텍스트와 레이블을 전처리하는 함수를 정의합니다.\n",
    "def preprocessing(batch):\n",
    "    label_list, text_list = [], []\n",
    "    \n",
    "    # 람다 함수를 사용해서 배치값을 차례대로 변환합니다.\n",
    "    for (_label, _text) in batch:\n",
    "        # 레이블에 람다 함수를 적용합니다.\n",
    "        label_list.append(label_transform(_label))\n",
    "        # 텍스트에 람다 함수를 적용합니다.\n",
    "        text_list.append(torch.tensor(text_transform(_text)))\n",
    "        \n",
    "    # 가장 긴 문장을 기준으로 정수 인코딩 된 문장의 길이를 통일합니다.\n",
    "    data = pad_sequence(text_list)\n",
    "    target = torch.tensor(label_list)\n",
    "    \n",
    "    # 전처리 결과를 반환합니다.\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "KMkwXgY1lUXl"
   },
   "outputs": [],
   "source": [
    "# IMDB 데이터세트를 학습 세트와 테스트 세트로 나눠서 불러옵니다.\n",
    "train_dataset, test_dataset = datasets.IMDB(split=('train', 'test'))\n",
    "\n",
    "# preprocessing 함수를 적용하여 학습 세트 데이터로더와 테스트 세트 데이터로더를 만듭니다.\n",
    "train_loader = DataLoader(list(train_dataset), batch_size=8, shuffle=True, collate_fn=preprocessing)\n",
    "test_loader = DataLoader(list(test_dataset), batch_size=8, shuffle=False, collate_fn=preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Kfzf_HslGa8R"
   },
   "outputs": [],
   "source": [
    "# LSTM 모델 클래스를 정의합니다.\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # 모델 구조를 정의합니다.\n",
    "        self.embed = nn.Embedding(vocab_size, 16)\n",
    "        self.cell = nn.LSTM(16, 16)\n",
    "        self.fc = nn.Linear(16, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    # 순전파를 정의합니다.\n",
    "    def forward(self, X):\n",
    "        out = self.embed(X)\n",
    "        out, (hidden_state, cell_state) = self.cell(out)\n",
    "        out = self.fc(hidden_state.view(-1, 16))\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "C94z4efYK2KD"
   },
   "outputs": [],
   "source": [
    "# 그래픽 카드 사용이 가능할 경우 그래픽 카드로 연산하도록 설정합니다.\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# LSTM 모델 객체를 생성합니다.\n",
    "vocab_size = len(vocabulary)\n",
    "model = LSTM(vocab_size).to(device)\n",
    "\n",
    "# 이진 크로스 엔트로피(Binary Cross Entropy Error) 손실 함수 객체를 생성합니다.\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# 아담 옵티마이저 객체를 생성합니다.\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "lvqVEySFwjyg"
   },
   "outputs": [],
   "source": [
    "# 학습 함수를 정의합니다.\n",
    "def train(model, criterion, optimizer, loader):\n",
    "    # 현재 에포크의 오차와 정확도를 저장할 변수를 생성합니다.\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    # 모델을 학습 모드로 설정합니다.\n",
    "    model.train()\n",
    "    # 배치 학습을 실행합니다.\n",
    "    for X_batch, y_batch in loader:\n",
    "        # 입력 데이터와 타깃을 준비합니다.\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device).float().view(-1, 1)\n",
    "        # 기울기를 초기화합니다.\n",
    "        optimizer.zero_grad()\n",
    "        # 모델을 사용해 타깃을 추론합니다.\n",
    "        hypothesis = model(X_batch)\n",
    "        # 손실 함수로 오차를 계산합니다.\n",
    "        loss = criterion(hypothesis, y_batch)\n",
    "        # 기울기를 계산합니다.\n",
    "        loss.backward()\n",
    "        # 경사 하강법으로 가중치를 수정합니다.\n",
    "        optimizer.step()\n",
    "        # 정확도를 계산합니다.\n",
    "        acc = ((hypothesis >= 0.5) == y_batch).float().mean()\n",
    "        # 현재 배치의 오차와 정확도를 저장합니다.\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    # 현재 에포크의 오차와 정확도를 반환합니다.\n",
    "    return epoch_loss / len(loader), epoch_acc / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "_S_sjAtTErMB"
   },
   "outputs": [],
   "source": [
    "# 평가 함수를 정의합니다.\n",
    "def evaluate(model, criterion, loader):\n",
    "    # 현재 에포크의 오차와 정확도를 저장할 변수를 생성합니다.\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    # 모델을 평가 모드로 설정합니다.\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # 배치 단위로 추론을 실행합니다.\n",
    "        for X_batch, y_batch in loader:\n",
    "            # 입력 데이터와 타깃을 준비합니다.\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device).float().view(-1, 1)\n",
    "            # 모델을 사용해 타깃을 추론합니다.\n",
    "            hypothesis = model(X_batch)\n",
    "            # 손실 함수로 오차를 계산합니다.\n",
    "            loss = criterion(hypothesis, y_batch)\n",
    "            # 정확도를 계산합니다.\n",
    "            acc = ((hypothesis >= 0.5) == y_batch).float().mean()\n",
    "            # 현재 배치의 오차와 정확도를 저장합니다.\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    # 현재 에포크의 오차와 정확도를 반환합니다.\n",
    "    return epoch_loss / len(loader), epoch_acc / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LOPQn5LBErMC",
    "outputId": "2b178a0b-3f8d-4be4-e449-5147c8c18148"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.693, acc: 0.50, test_loss: 0.692, test_acc: 0.507\n",
      "epoch: 1, loss: 0.690, acc: 0.51, test_loss: 0.689, test_acc: 0.513\n",
      "epoch: 2, loss: 0.665, acc: 0.59, test_loss: 0.644, test_acc: 0.664\n",
      "epoch: 3, loss: 0.632, acc: 0.65, test_loss: 0.643, test_acc: 0.633\n",
      "epoch: 4, loss: 0.603, acc: 0.69, test_loss: 0.636, test_acc: 0.641\n",
      "epoch: 5, loss: 0.582, acc: 0.72, test_loss: 0.605, test_acc: 0.695\n",
      "epoch: 6, loss: 0.559, acc: 0.74, test_loss: 0.589, test_acc: 0.711\n",
      "epoch: 7, loss: 0.591, acc: 0.67, test_loss: 0.573, test_acc: 0.738\n",
      "epoch: 8, loss: 0.526, acc: 0.75, test_loss: 0.500, test_acc: 0.774\n",
      "epoch: 9, loss: 0.407, acc: 0.83, test_loss: 0.431, test_acc: 0.812\n",
      "epoch: 10, loss: 0.337, acc: 0.87, test_loss: 0.412, test_acc: 0.834\n",
      "epoch: 11, loss: 0.287, acc: 0.89, test_loss: 0.389, test_acc: 0.835\n",
      "epoch: 12, loss: 0.247, acc: 0.91, test_loss: 0.388, test_acc: 0.842\n",
      "epoch: 13, loss: 0.241, acc: 0.91, test_loss: 0.393, test_acc: 0.852\n",
      "epoch: 14, loss: 0.181, acc: 0.93, test_loss: 0.391, test_acc: 0.845\n",
      "epoch: 15, loss: 0.165, acc: 0.94, test_loss: 0.395, test_acc: 0.857\n",
      "epoch: 16, loss: 0.138, acc: 0.95, test_loss: 0.431, test_acc: 0.852\n",
      "epoch: 17, loss: 0.124, acc: 0.96, test_loss: 0.430, test_acc: 0.856\n",
      "epoch: 18, loss: 0.148, acc: 0.95, test_loss: 0.389, test_acc: 0.849\n",
      "epoch: 19, loss: 0.096, acc: 0.97, test_loss: 0.452, test_acc: 0.855\n",
      "epoch: 20, loss: 0.075, acc: 0.98, test_loss: 0.486, test_acc: 0.853\n",
      "epoch: 21, loss: 0.066, acc: 0.98, test_loss: 0.537, test_acc: 0.827\n",
      "epoch: 22, loss: 0.062, acc: 0.98, test_loss: 0.531, test_acc: 0.852\n",
      "epoch: 23, loss: 0.052, acc: 0.99, test_loss: 0.535, test_acc: 0.854\n",
      "epoch: 24, loss: 0.049, acc: 0.99, test_loss: 0.586, test_acc: 0.847\n"
     ]
    }
   ],
   "source": [
    "# 25회에 걸쳐 모델을 학습합니다.\n",
    "n_epochs = 25\n",
    "for epoch in range(n_epochs):\n",
    "    # 모델을 학습시킵니다.\n",
    "    loss, acc = train(model, criterion, optimizer, train_loader)\n",
    "    # 모델을 평가합니다.\n",
    "    test_loss, test_acc = evaluate(model, criterion, test_loader)\n",
    "    \n",
    "    # 현재 에포크의 학습 결과를 출력합니다.\n",
    "    print('epoch: {}, loss: {:.3f}, acc: {:.2f}, test_loss: {:.3f}, test_acc: {:.3f}'.format(\n",
    "        epoch, loss, acc, test_loss, test_acc\n",
    "    ))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "3.5.3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
